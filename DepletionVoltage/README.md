# DEPLTETION VOLTAGE

HV scans must be run periodically in order to determine the depletion voltage for each channel (defined as HV value at the plateau of triple-coincidence rate) and the operational voltage must be maintained above this value.

## AUTOMATED SCANS

In August 2018, a script [cms_cen_dcs_2/cms_brm_dcs_1cms_plt_dcs_1/CMS_PLT_HVScan/CMS_PLT_HVScan.ctl](https://gitlab.cern.ch/CMSDCS/DCS/DCS_COMPONENTS/BRM/CMS_PLT_HVScan/) was developed to automate the HV scans. It writes an output log file to [\\\\cern.ch\\dfs\\Users\\c\\cmsdcs\\Public\\DCSFiles\\PLTHVScan\\](https://dfsweb.web.cern.ch/dfsweb/Services/DFS/DFSBrowser.aspx/Users/c/cmsdcs/Public/DCSFiles/PLTHVScan/) for each scan. The 'processScanLogs.py' script will process all scan log files present in the path specified by the 'dataDir' variable in the 'main()' function. Note that the script requires python 3.6 or above, pandas, and matplotlib. It also requires brilcalc. Please see [setup_pltoffline.sh](https://github.com/cmsplt/PLTOffline/blob/master/setup_pltoffline.sh) to set up a python virtual environment, if needed.

The HV scan log files contain comma-separated fields in the following format:
* timestamp
* #M (lines missing this marker in the second field correspond to metadata, such as setpoint)
* vMon (per-channel)
* iMon (per-channel)
* rate (per-channel)
* avgRate

Note that the channel order in the log files is hard-coded to 'HpFT0,HpFT1,HpFT2,HpFT3,HpNT0,HpNT1,HpNT2,HpNT3,HmFT0,HmFT1,HmFT2,HmFT3,HmNT0,HmNT1,HmNT2,HmNT3', which corresponds to readouts channels [12,13,14,15,8,9,10,11,4,5,6,7,0,1,2,3]. The script parses each log file into a pandas.DataFrame. In order to compensate for the fact that the instantaneous luminosity will generally decrease during a scan (and other changes, such as LHC optimizations, may also occur), the per-channel rates are normalized using the rate (average pileup) from a reference luminometer. These rates are obtained using brilcalc for the time of the HV scan and a DataFrame with HFET rates per lumi section is returned (BCM1F is queried if HFET data is unavailable). The PLT per-channel rates are simply divided by the HFET/BCM1F rate corresponding to the closest timestamp available.

The HV steps in the scan program are then determined by looking for more than four instances of per-channel vMon that round to the same integer value. For each step in the scan program, samples with per-channel vMon values within 2 V of the set point are considered. These samples are further required to have normalized per-channel rates within 5% of the last sample in the scan step. This requirement is motivated by the observation that the rate for some scan steps fluctuates initially until finally "settling" into a value. The median and standard deviation of the weighted per-channel rates are calculated for all samples passing the requirements.

The depletion voltage for each channel is then determined by calculating the percent change in weighted rate as the HV set point decreases. This strategy must be flexible enough to accommodate the wide variety of scan programs and resulting rate vs. HV curves, which other methods (such as fitting a function) were less successful at handling. In particular, the behavior at lower HV values is very often non-uniform due to time walk, which affects the efficiency and thus the rates. In addition, the rate at each scan point has to be compared to the rate for the previous two set points to account for outliers in the rate vs. HV curves. Finally, the calculated per-channel depletion voltage for all available HV scan logs is written to disk as a JSON file.

## MANUAL SCANS

Before the automated HV scan script was available, the HV scans were run by hand with with varying programs. In order to study the depletion voltage for all HV scans during Run 2, log data had to be collected from multiple sources and aggregated.

The CAEN mainframe was set up to log all changes in HV and LV. This information is available from the [cmsonline website](https://cmsonline.cern.ch/webcenter/portal/cmsonline/pages_services/brilinfo), from which the data can be queried and exported. In particular, the measured HV (vMon) was exported for all channels as a CSV file for 2015, 2016, 2017, and 2018 and is available under the 'CaenLogs/' directory. Note that new data is only logged if there is a change with respect to the previous value. The 'identifyScans.py' script was developed to parse these exported CSV files and identify all bias scans. Again, it requires python 3.6 or above, pandas, matplotlib, and brilcalc. The vMon CSV file is parsed into a pandas.DataFrame with per-channel vMon as columns and the respective timestamps as rows. Since log data is only present when the value of vMon changes, the parsed vMon data is merged with an empty pandas.DataFrame with a 10-second-frequency. Missing (Nan) values are filled to propagates the first valid observation forward until the next valid observation in each column, thus ensuring that the vMon data is available for any time queried. The script then identifies bias scans by grouping together any changes in vMon greater than 10 V that are spaced no more than 30 minutes apart. These values assume all scan programs will have HV set points more than 10 V apart and that each step will last no longer than 30 minutes. To further reduce the number of false positives (e.g. multiple HV ramps during a short period), the "scan candidates" are required to occur during periods of ADJUST or STABLE BEAMS. This requirement is enforced by querying brilcalc between the start and end timestamps of each scan candidate. Finally, the script plots vMon vs. time for all scan candidates. Any candidates that don't correspond to a bias scan or are not usable for whatever reason can be flagged as bad at this step (see 'removeBadScans()' function). Finally, the processed vMon data is exported to a pickle file for further processing.

Another python3 script (same dependencies, and also requires numpy), 'writeScanLogs.py' was developed in order to merge the processed vMon log data with PLT per-channel rates obtained from PLT workloop histogram files. For each scan candidate, the corresponding pickle file with vMon data is loaded (with functionality to allow cropping the "edges" of each scan candidate; see 'scanBoundary()' function) and a corresponding workloop histogram file(s) is identified based on the start timestamp of the scan candidate. Each histogram file is then processed using numpy and then read into a pandas.DataFrame. Since all workloop files are archived as compressed files, the gzip module is used to read the files. In order to avoid reading the entire file (often very large) into memory, the timestamp corresponding to the first line in the histogram file is read. From the the first timestamp in the histogram file, the number of bytes needed to seek to the start timestamp of the scan candidate is calculated. In addition, the number of bytes to read from the file is determined from the scan candidate's end timestamp. At this point, the histogram data corresponding to the scan candidate is read into memory as a numpy ndarray.

The workloop histogram files contain raw per-channel per-BX triple coincidence rates for every nibble. The per-channel rates are determined by simply aggregating the rate across all BXs, and they are then loaded into a pandas.DataFrame. Since the histogram files only log the number of seconds since midnight, multiple checks must be made. First, the date is determined from the histogram filename and the first timestamp in the histogram file is compared to the start timestamp of the scan candidate, in order to increment a day if midnight has rolled over from the time the histogram file was created until the start of the scan. This updated date is then incremented by one if midnight rolls over during the duration of the scan, which is accomplished by identifying any rows in the histogram data where the timestamp is smaller than the previous one. If found, the pandas.DataFrame is sliced at this index position and the date incremented by one for subsequent rows. With the parsed and processed vMon data and per-channel rate data on hand, the pandas.DataFrames are merged. The format and columns are organized in such as way that they match the log files written by the automated HV scan script and the combined data is written to disk as CSV log files.
